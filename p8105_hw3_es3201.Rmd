---
title: "Homework 3"
author: Eric Sun
output: github_document
editor_options: 
  chunk_output_type: console
---

Initial setup

```{r}
library(tidyverse)
library(p8105.datasets)
library(viridis)
library(ggridges)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

```

# Problem 1

```{r}
data("instacart")
```

This dataset contains `r nrow(instacart)` rows and ... columns. 

Observations are the level of items in orders by user. There are user / order variables -- user ID, order ID, order day, and order hour. There are also item variables -- name, aisle, department, and some numeric codes. 

How many aisles, and which are most items from?

```{r}
instacart %>% 
	count(aisle) %>% 
	arrange(desc(n))
```

Let's make a plot
```{r}
instacart %>% 
	count(aisle) %>% 
	filter(n > 10000) %>% 
	mutate(
		aisle = factor(aisle),
		aisle = fct_reorder(aisle, n)
	) %>% 
	ggplot(aes(x = aisle, y = n)) + 
	geom_point() + 
	theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

Let's make a table
```{r}
instacart %>% 
	filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
	group_by(aisle) %>% 
	count(product_name) %>% 
	mutate(rank = min_rank(desc(n))) %>% 
	filter(rank < 4) %>% 
	arrange(aisle, rank) %>% 
	knitr::kable()
```

Apples vs ice cream
```{r}
instacart %>% 
	filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
	group_by(product_name, order_dow) %>% 
	summarize(mean_hour = mean(order_hour_of_day)) %>% 
	pivot_wider(
		names_from = order_dow,
		values_from = mean_hour
	)
```

# Problem 2

Load, tidy, wrangle data; include a weekday vs weekend variable; and encode data with reasonable variable classes
```{r load p2 data}
accel_df = read_csv("./data/accel_data.csv") %>%
  pivot_longer(activity.1:activity.1440, names_to = "minute", names_prefix="activity.", values_to = "count") %>%
  mutate(day=factor(day,c("Monday", "Tuesday", "Wednesday", "Thursday","Friday","Saturday","Sunday"))) %>%
  mutate(weekend=case_when(
    day %in% c("Saturday","Sunday") ~ "TRUE",
    day %in% c("Monday", "Tuesday", "Wednesday", "Thursday","Friday") ~ "FALSE")) %>%
  mutate(weekend=as.logical(weekend)) %>%
  mutate(minute=as.numeric(minute))
accel_df
```

The accel_df database contains `r nrow(accel_df)` rows and `r ncol(accel_df)` columns. The variables describe the timing of the observations and the amount of activity for each observation.

Aggregate across minutes to create a total activity variable for each day, and create a table showing these totals
```{r}
accel_df %>%
  group_by(day) %>%
  summarize(sum=sum(count))
```

After creating the table of activity by day, you can see that this patient is most active from Wednesday-Friday and least active on Saturday. 

Plot of activity for each day
```{r}
accel_df %>%
  mutate(hour=case_when(
    minute< 60 ~ 0,
    minute< 120 ~ 1,
    minute< 180 ~ 2,
    minute< 240 ~ 3,
    minute< 300 ~ 4,
    minute< 360 ~ 5,
    minute< 420 ~ 6,
    minute< 480 ~ 7,
    minute< 540 ~ 8,
    minute< 600 ~ 9,
    minute< 660 ~ 10,
    minute< 720 ~ 11,
    minute< 780 ~ 12,
    minute< 840 ~ 13,
    minute< 900 ~ 14,
    minute< 960 ~ 15,
    minute< 1020 ~ 16,
    minute< 1080 ~ 17,
    minute< 1140 ~ 18,
    minute< 1200 ~ 19,
    minute< 1260 ~ 20,
    minute< 1320 ~ 21,
    minute< 1380 ~ 22,
    minute<=1440 ~ 23
  )) %>%
  ggplot(aes(x=hour, y=count, color=day, fill=day)) +
  geom_smooth(alpha=.5)
```

Based on the graph, the patient is least active at night and starts to increase activity around hour 3. He hits a peak around hour 10 and a second peak around hour 20, after which his activity decreases sharply.

# Problem 3

Load dataset
```{r}
data("ny_noaa")
```

Describe dataset
```{r}
ny_noaa %>%
  select(prcp:tmin) %>%
  summarise_all(funs(sum(is.na(.))))
```


The ny_noaa database contains `r nrow(ny_noaa)` rows and `r ncol(ny_noaa)` columns. For each date and weather station, the dataset contains the amount of precipitation (prcp), snowfall (snow), snow depth (snwd), maximum temperature (tmax) and minimum temperature (tmin). There are a significant amount of missing values with `r 145838/nrow(ny_noaa)` of prcp, `r 381221/nrow(ny_noaa)` of snow, `r 591786/nrow(ny_noaa)` of snwd, `r 1134358/nrow(ny_noaa)` of tmax and `r 1134420/nrow(ny_noaa)` of tmin missing.

Clean data, separate date, change units, and find most common observed value of snowfall
```{r}
ny_noaa = ny_noaa %>%
  mutate(prcp=prcp/10) %>%
  mutate(tmax=as.numeric(tmax)/10) %>%
  mutate(tmin=as.numeric(tmin)/10) %>%
  separate(date,into=c("year","month","day"),sep="-",remove=TRUE,convert=TRUE,extra="warn",fill="warn")

ny_noaa %>%
  group_by(snow)%>%
  summarize(n_obs=n()) %>%
  mutate(snow_rank=min_rank(desc(n_obs))) %>%
  filter(snow_rank<=5)

```
The most commonly observed values of snowfall are 0 by far, followed by 25, 13, and 51. 0 is most common because it is more likely than not not snowing on any given day. The other values are close to .5, 1, and 2 inches which is probably the unit of measure that is being used in the US.

Two-panel plot showing the average max temperature in January and in July in each station across years
```{r}
ny_noaa%>%
  select(id,year,month,tmax) %>%
  filter(month==c("1","7")) %>%
  drop_na(tmax) %>%
  group_by(id,year,month) %>%
  summarize(average_max_temp=mean(tmax)) %>%
  ggplot(aes(x=year,y=average_max_temp, group=id, color=id))+
  geom_point() +
  geom_path()+
  facet_grid(. ~ month) +
  theme(legend.position="none")
```

The main discernible pattern is that the average max temperatures in January are lower than in July. The temperatures also oscillate, and roughly every 5 years it hits a relative peak or relative low before moving in the opposite direction.


